{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_df = pd.read_csv(\"food_training_df.csv\")\n",
    "food_df['Topic'] = food_df['Topic'].replace(to_replace=['Food Effect', 'Non Food Effect'], value = [1,0]).astype(float)\n",
    "food_df[['Topic', 'Data_Source']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm_food_df = food_df[food_df['Data_Source'] == 'DailyMed'].sample(n = 1200, random_state = 1234)\n",
    "df_food_df = food_df[food_df['Data_Source'] == 'DrugsFDA'].sample(n = 1200, random_state = 1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dm_food_df['Topic'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_food_df['Topic'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df):\n",
    "    train_df = df.sample(frac = 0.8, random_state = 1234)\n",
    "    test_df = df.drop(train_df.index).reset_index(drop=True)\n",
    "    train_df = train_df.reset_index(drop = True)\n",
    "    print('{},{}'.format(str(len(train_df)), str(len(test_df))))\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm_train_df, dm_test_df = prepare_data(dm_food_df)\n",
    "print(dm_train_df['Topic'].value_counts())\n",
    "print(dm_test_df['Topic'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_df, df_test_df = prepare_data(df_food_df)\n",
    "print(df_train_df['Topic'].value_counts())\n",
    "print(df_test_df['Topic'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmdf_train_df = pd.concat([dm_train_df, df_train_df])\n",
    "print(dmdf_train_df['Topic'].value_counts())\n",
    "dmdf_test_df = pd.concat([dm_test_df, df_test_df])\n",
    "print(dmdf_test_df['Topic'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score, recall_score, precision_score\n",
    "def report_results(A, B):\n",
    "    A_name = A.name\n",
    "    B_name = B.name\n",
    "    \n",
    "    df = pd.DataFrame({'A':A,\n",
    "                       'B':B})\n",
    "    df = df.dropna()\n",
    "    A = df['A']\n",
    "    B = df['B']\n",
    "    \n",
    "    acc = accuracy_score(B, A)\n",
    "    f1 = f1_score(B, A)\n",
    "    prec = precision_score(B, A)\n",
    "    rec = recall_score(B, A)\n",
    "    ROC = roc_auc_score(B, A)\n",
    "    \n",
    "#     print('Candidate: '+A_name+' | Ground Truth: '+B_name+'\\n')\n",
    "    print('accuracy: %0.4f \\nprecision: %0.4f \\nrecall: %0.4f \\nF1 score: %0.4f \\nROC AUC: %0.4f \\n' % (acc, prec, rec, f1, ROC))\n",
    "    return prec, rec, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    \n",
    "def perform_model(train_df, test_df):\n",
    "    \n",
    "    tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), max_features=100)\n",
    "    tfidf.fit(train_df['Paragraph']) # .toarray()  \n",
    "    x_train = tfidf.transform(train_df['Paragraph'])\n",
    "    y_train = train_df['Topic']\n",
    "    x_test = tfidf.transform(test_df['Paragraph'])\n",
    "    y_test = test_df['Topic']\n",
    "    \n",
    "    model_lc = LogisticRegression()\n",
    "    model_lc.fit(x_train, y_train)\n",
    "    test_df['Prediction'] = model_lc.predict(x_test)\n",
    "    print('Logistic Regression\\n')\n",
    "    lr_prec, lr_rec, lr_f1 = report_results(test_df['Prediction'], test_df['Topic'])\n",
    "    incorrect = test_df[test_df['Prediction'] != test_df['Topic']]\n",
    "    print('incorrect: ' + str(len(incorrect)))\n",
    "    \n",
    "    model_ls = LinearSVC()\n",
    "    model_ls.fit(x_train, y_train)\n",
    "    test_df['Prediction'] = model_ls.predict(x_test)\n",
    "    print('\\nLinear SVC\\n')\n",
    "    ls_prec, ls_rec, ls_f1 = report_results(test_df['Prediction'], test_df['Topic'])\n",
    "    incorrect = test_df[test_df['Prediction'] != test_df['Topic']]\n",
    "    print('incorrect: ' + str(len(incorrect)))\n",
    "    \n",
    "    model_rf = RandomForestClassifier(random_state=100)\n",
    "    model_rf.fit(x_train, y_train)\n",
    "    test_df['Prediction'] = model_rf.predict(x_test)\n",
    "    print('\\nRandom Forest\\n')\n",
    "    rf_prec, rf_rec, rf_f1 = report_results(test_df['Prediction'], test_df['Topic'])\n",
    "    incorrect = test_df[test_df['Prediction'] != test_df['Topic']]\n",
    "    print('incorrect: ' + str(len(incorrect)))\n",
    "    \n",
    "    return lr_prec, lr_rec, lr_f1, ls_prec, ls_rec, ls_f1, rf_prec, rf_rec, rf_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source_result_df = pd.DataFrame(columns=['F1'])\n",
    "method_result_df = pd.DataFrame(columns=['Precision', 'Recall', 'F1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_prec, lr_rec, lr_f1, ls_prec, ls_rec, ls_f1, rf_prec, rf_rec, rf_f1 = perform_model(dmdf_train_df, dmdf_test_df)\n",
    "data_source_result_df.loc['dm+df_dm+df'] = [rf_f1]\n",
    "method_result_df.loc['Logistic Regression'] = [lr_prec, lr_rec, lr_f1]\n",
    "method_result_df.loc['Linear SVC'] = [ls_prec, ls_rec, ls_f1]\n",
    "method_result_df.loc['Random Forest'] = [rf_prec, rf_rec, rf_f1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_prec, lr_rec, lr_f1, ls_prec, ls_rec, ls_f1, rf_prec, rf_rec, rf_f1 = perform_model(dm_train_df, dmdf_test_df)\n",
    "data_source_result_df.loc['dm_dm+df'] = [rf_f1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_prec, lr_rec, lr_f1, ls_prec, ls_rec, ls_f1, rf_prec, rf_rec, rf_f1 = perform_model(df_train_df, dmdf_test_df)\n",
    "data_source_result_df.loc['df_dm+df'] = [rf_f1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_prec, lr_rec, lr_f1, ls_prec, ls_rec, ls_f1, rf_prec, rf_rec, rf_f1 = perform_model(dmdf_train_df, dm_test_df)\n",
    "data_source_result_df.loc['dm+df_dm'] = [rf_f1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_prec, lr_rec, lr_f1, ls_prec, ls_rec, ls_f1, rf_prec, rf_rec, rf_f1 = perform_model(dm_train_df, dm_test_df)\n",
    "data_source_result_df.loc['dm_dm'] = [rf_f1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_prec, lr_rec, lr_f1, ls_prec, ls_rec, ls_f1, rf_prec, rf_rec, rf_f1 = perform_model(df_train_df, dm_test_df)\n",
    "data_source_result_df.loc['df_dm'] = [rf_f1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_prec, lr_rec, lr_f1, ls_prec, ls_rec, ls_f1, rf_prec, rf_rec, rf_f1 = perform_model(dmdf_train_df, df_test_df)\n",
    "data_source_result_df.loc['dm+df_df'] = [rf_f1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_prec, lr_rec, lr_f1, ls_prec, ls_rec, ls_f1, rf_prec, rf_rec, rf_f1 = perform_model(dm_train_df, df_test_df)\n",
    "data_source_result_df.loc['dm_df'] = [rf_f1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_prec, lr_rec, lr_f1, ls_prec, ls_rec, ls_f1, rf_prec, rf_rec, rf_f1 = perform_model(df_train_df, df_test_df)\n",
    "data_source_result_df.loc['df_df'] = [rf_f1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source_result_df.to_csv('data_source_result_df.csv', index=True)\n",
    "method_result_df.to_csv('method_result_df.csv', index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
